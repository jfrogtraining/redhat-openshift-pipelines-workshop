[
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/",
	"title": "JFrog Platform on Red Hat Openshift Workshop",
	"tags": [],
	"description": "",
	"content": "JFrog Platform on Red Hat Openshift Welcome In this workshop you will learn about the JFrog Platform and how to leverage Artifactory and Xray for managing your Software Development Lifecycle (SDLC) and bring DevOps to the cloud native Red Hat Openshift.\nLearning Objectives  Understand the roles of Artifactory and Xray in your software delivery life cycle (SDLC). Use Local, Remote and Virtual Repositories in Artifactory. Publish and promote your artifacts and Build Info. Scan your artifacts and builds for security vulnerabilities. Build and deploy your application onto Red Hat Openshift.  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\n "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/1_prerequisites.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "The following items are required for this workshop.\n  Red Hat Openshift account - If you are at an Red Hat Openshift event, an account will be provided. Otherwise, go [here](https://Red Hat Openshift.microsoft.com/en-us/free/) to create an Red Hat Openshift account.\n  JFrog Platform instance - Use the JFrog Platform Cloud Free Tier to get your own JFrog Platform instance with Artifactory and Xray.\n  Have a notepad available for saving important workshop data.\n  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/2_devops_cloud.html",
	"title": "DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The goal of DevOps is to allow your development teams to deliver quality software faster to your customers through continuous process improvement, leveraging the best of breed development tools and infrastructure, and utilizing software development and IT operations best practices. Your team must deliver software faster than your competitors in order to get features and fixes to your customers sooner. JFrog terms this ideal as liquid software.\n Looking forward, as release cycles get shorter and microservices get smaller, we can imagine a world in which at any one time, our systems’ software is being updated. Effectively, software will become liquid in that products and services will be connected to “software pipes” that constantly stream updates into our systems and devices; liquid software continuously and automatically updating our systems with no human intervention.\n\u0026ndash; JFrog (2017), A Vision of Liquid Software, Retrieved from https://jfrog.com/whitepaper/a-vision-of-liquid-software/ A critical aspect of DevOps is infrastructure. Cloud computing infrastructure has allowed DevOps to advance and come closer to realizing liquid software. Cloud computing has allowed development teams to build these software pipes by:\n Using ephemeral cloud infrastructure to scale their development process and software delivery at levels not achievable with on-premise infrastructure. Providing applications on a global scale with real-time response and resiliency. Leveraging new cloud services in their application and software development processes to improve the quality, security and delivery of their applications. Allowing multi-discipline teams to collaborate in the cloud across the software lifecycle to ensure quality, security, velocity and scale of applications.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/3_workshop.html",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will demonstrate DevOps in the cloud with Red Hat Openshift and JFrog. We will build and deploy a containerized NPM application. Using the JFrog Platform and the JFrog CLI, we will compile our code, build our NPM package, execute a docker build and push, security scan the image and publish it to a repository. We will then deploy the image and serve the application with Red Hat Openshift AKS.\n![Hands On Diagram](/images/Red Hat Openshift-diagram-steps.png)\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup.html",
	"title": "Workshop Setup",
	"tags": [],
	"description": "",
	"content": "Before we get started on building, publishing and deploying our NPM application, we must set up our workshop environment. In this setup section, we will:\n Set up our Red Hat Openshift account. Configure the Red Hat Openshift Cloud Shell. Clone our workshop GitHub repository which contains our code. Install and configure the JFrog CLI. Prepare our JFrog Platform instance.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/5_build_publish_app.html",
	"title": "Build and Publish the App",
	"tags": [],
	"description": "",
	"content": "The JFrog CLI is a powerful tool that you can use in your CI/CD process and toolchain. It can be used to build code and publish artifacts while collecting valuable build information along the way. It greatly simplifies the publishing of the build artifacts and the build info to JFrog Artifactory. It is commonly used in automation scripts and with CI/CD software tools. In the next steps, we will execute a CI/CD process with JFrog CLI commands to demonstrate how to build and publish with NPM and Docker.\nIn this workshop, we use NPM and Docker, but the JFrog Platform is a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.\n "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/6_view_results.html",
	"title": "View Results in JFrog",
	"tags": [],
	"description": "",
	"content": "We have built and published our NPM package and Docker image. Let\u0026rsquo;s view these results in JFrog Artifactory.\n  Go to your JFrog Platform instance and switch to the Packages view in Artifactory. Go to Artifactory ► Packages.\n  Type workshop-app and search. This will show the NPM package that was published with the JFrog CLI.\n  Click on it to view the details.   Go back to the Packages view and search for npm-app. This shows the Docker image that was published.\n  Click on the docker npm-app listing.   This will show a list of the versions. Click on the latest version that was built.   In the Xray Data tab, view the security violations. License violations are available in the JFrog Platform Pro and Enterprise tiers.   Click on any violation to see the details and impact in the Issue Details tab.   Scroll down to the References section to access links to documentation that can help you remediate the issue. In many cases, you just need to update the component and Xray will indicate this.   Xray supports all major package types, understands how to unpack them, and uses recursive scanning to see into all of the underlying layers and dependencies of components, even those packaged in Docker images, and zip files. The comprehensive vulnerability intelligence databases are constantly updated giving the most up-to-date understanding of the security and compliance of your binaries.\n Close the Issue Details tab. View the Docker configuration for the image in the Docker Layers tab. On the Builds tab, click on npm_build in the list.  Then click on your most recent build. In the Published Modules tab, view the set of artifacts and dependencies for your build.   Our JFrog CLI CI/CD \u0026ldquo;pipeline\u0026rdquo; provided an overview of a typical build, docker build and push, security scan and promotion process using Artifactory and Xray.\nNext, we will deploy your docker image from the \u0026ldquo;staging\u0026rdquo; repository using Red Hat Openshift Kubernetes Service (AKS).\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/7_deploy_openshift.html",
	"title": "Deploy on Red Hat Openshift",
	"tags": [],
	"description": "",
	"content": "We are now ready to deploy your image with Red Hat Openshift Kubernetes.\n Execute the following command to get connectivity to the cluster.  az aks get-credentials \\ --resource-group jfrog-Red Hat Openshift-workshop \\ --name jfrog-Red Hat Openshift-workshop-cluster To confirm connectivity, execute the following command to list the Kubernetes nodes.  kubectl get nodes\nFirst, let\u0026rsquo;s create a namespace to provide isolation. Execute the following.  kubectl create namespace jfrog-Red Hat Openshift-workshop\nNext, we need to set our Artifactory registry credentials in order to pull the NPM application image. We will do this my creating Kubernetes secrets to store these. Execute the following command. Substitute your server name and JFrog Platform credentials (username and API key).  kubectl create secret docker-registry regcred --namespace jfrog-Red Hat Openshift-workshop --docker-server=$JFROG_SERVER_ID --docker-username=$JFROG_USER --docker-password=$JFROG_APIKEY\nWe will use a Kubernetes deployment manifest to deploy the NPM application image. First, we must make a substitution in the file for the image that we want to deploy.  sed \u0026quot;s|imageName|$IMAGE_NAME|g\u0026quot; deployment.yaml \u0026gt; my-deployment.yaml\nNow we can deploy with the following command.  kubectl apply -f my-deployment.yaml --namespace jfrog-Red Hat Openshift-workshop\nExecute the following to see your deployed pod.  kubectl get pods --namespace jfrog-Red Hat Openshift-workshop\nYou should see you npm-app pod.\nNow let\u0026rsquo;s get the external IP so that we can view your application. Execute the following.  kubectl get services --namespace jfrog-Red Hat Openshift-workshop\nThis will provide the EXTERNAL-IP.\nIn your browser, go to https://\u0026lt;EXTERNAL-IP\u0026gt; to view your deployed web application. Click through the self-signed certificate warning. You should see the following web application.  Congratulations! You have used Red Hat Openshift AKS to deploy the image that you built with the JFrog Platform.\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/8_conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "In this workshop, we used the JFrog Platform to build an application, manage the artifacts, scan the artifacts for security vulnerabilities and license compliance, and publish the artifacts of your application to a staging repository. Then we used Red Hat Openshift AKS to deploy your application so that end-users can access it. The JFrog Platform and Red Hat Openshift AKS to demonstrate how you can build a DevOps cloud platform on Red Hat Openshift to delivery your software to your end-users. This modernizes your software delivery life cycle enabling your organization to deliver quality software continuously by leveraging advanced cloud services and elastic infrastructure.\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/2_devops_cloud/21_continuous_integration_and_delivery.html",
	"title": "Continuous Integration and Delivery",
	"tags": [],
	"description": "",
	"content": "Continuous integration and delivery (CI/CD) is the process for which your software components are built from code, integrated, tested, released, deployed and ultimately delivered to end-users. CI/CD pipelines are the software assembly line that orchestrates the building of your software. This CI/CD pipeline line requires infrastructure. Cloud computing has allowed this infrastructure to become dynamic and ephemeral. On cloud infrastructure, your CI/CD pipelines scale up and down to meet your software delivery demands. It saves costs by providing the right amount of cloud infrastructure just as it is needed. This is further realized by using cloud-native technologies like Kubernetes and extending across clouds and on-premise datacenters. The following are some Red Hat Openshift cloud technologies that CI/CD pipelines can utilize:\n Red Hat Openshift Virtual Machines can be used as CI/CD pipeline nodes that can be dynamically spun up and down to execute pipeline tasks. Red Hat Openshift Spot Virtual Machines can dramatically lower costs by utilizing spare capacity nodes for CI/CD pipeline tasks. Red Hat Openshift Kubernetes Service (AKS) can provide a Kubernetes-based CI/CD worker node pools and allow more efficient use of compute resources. Red Hat Openshift Stack can allow you to span your CI/CD pipelines from your on-premise datacenter to the cloud for hybrid and migration use cases.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/2_devops_cloud/22_binary_repository_management.html",
	"title": "Binary Repository Management",
	"tags": [],
	"description": "",
	"content": "A Binary Repository Manager is a software hub that simplifies the development process for different teams across an organization by helping them to collaborate on building coherent and compatible software components. It does this by centralizing the management of all the binary artifacts generated and used by the organization, thereby overcoming the incredible complexity arising from diverse binary artifact types, their position in the overall workflow and the set of dependencies between them.\nSome of the many benefits of using a Binary Repository Manager are:\n Reliable and consistent access to remote artifacts. Reduced network traffic and optimized builds. Tight integration with build ecosystems. Custom handling of artifacts to comply with any organization’s requirements. Security and access control to artifacts and repositories. Manage licensing requirements and open source governance for use of software components. Distributing and sharing artifacts across an organization. System stability and reliability with high availability architecture. Smart search for binaries. Advanced maintenance and monitoring tools.  Cloud infrastructure has provided additional benefits. With the cloud, binary repositories can now:\n Enable replication and resiliency through the use of global data centers. Provide lower latency and improved network performance by being available closer to end-users. Provide their services at the edge of the network regionally and globally to edge devices. Utilize cloud storage for reduced costs, scalability and lower maintenance. Leverage cloud services such as security vulnerability databases to extend their functionality.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/2_devops_cloud/23_dev_sec_ops.html",
	"title": "DevSecOps",
	"tags": [],
	"description": "",
	"content": "Any security issue identified by a security scanning may be reviewed by a small security team that may lack the technical knowledge. This challenge can be reduced by shifting left to the developer and operations teams, making them also responsible for security and compliance. This moves security earlier in the software delivery process. Source code, dependency and artifact security scanning are some examples of moving security into the development process. Implementing the identification of security issues earlier in the CI/CD pipeline, as well as automating security and compliance policies in the Software Development Lifecycle (SDLC), rather than using manual processes, is crucial. Moreover, organizations that leave the Sec out of DevOps, may face security and compliance issues that are closer to their release, resulting in additional costs for remediating such issues.\nAs you move your SDLC to the cloud, your DevSecOps strategy must also adapt to the cloud. As discussed previously, binary repository managers that scale globally across cloud data centers require DevSecOps tools that will likewise scale and adjust. An enterprise scale software delivery system with multiple development teams, end users and devices mean more entry points for potential security and compliance issues. Therefore, it is critical that your SLDC is well-integrated with your DevSecOps system.\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/2_devops_cloud/24_jfrog_platform_overview.html",
	"title": "JFrog Platform for DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The JFrog Platform is designed to meet the growing needs of companies to develop and distribute software in the cloud. It provides DevOps teams with the tools needed to create, manage, secure and deploy software with ease. These tools cover everything from continuous integration and delivery (CI/CD), binary repository management, artifact maturity, security and vulnerability protection (DevSecOps), release management, analytics and distribution.\nJFrog Artifactory is an Artifact Repository Manager that fully supports software packages created by any language or technology. Furthermore, it integrates with all major CI/CD and DevOps tools to provide an end-to-end, automated solution for tracking artifacts from development to production.\nJFrog Xray provides universal artifact analysis, increasing visibility and performance of your software components by recursively scanning all layers of your organization’s binary packages to provide radical transparency and unparalleled insight into your software architecture.\nJFrog Distribution empowers DevOps to distribute and continuously update remote locations with release-ready binaries.\nJFrog Artifactory Edge accelerates and provides control of release-ready binary distribution through a secure distributed network and edge nodes.\nJFrog Mission Control and Insight is your DevOps dashboard solution for managing multiple services of Artifactory, Xray, Edge and Distribution.\nJFrog Access with Federation provides governance to the distribution of artifacts by managing releases, permissions and access levels.\nJFrog Pipelines helps automate the non-human part of the whole software development process with continuous integration and empowers teams to implement the technical aspects of continuous delivery.\nAll of these JFrog Platform components are designed and developed to work together out-of-the-box with minimal configuration. Management and monitoring of your software delivery lifecycle from build to distribution is accessible though a central, unified user interface. The JFrog platform is enterprise ready with your choice of on-prem, cloud, multi-cloud or hybrid deployments that scale as you grow.\n "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/41_red_hat_openshift_setup.html",
	"title": "Red Hat Openshift Setup",
	"tags": [],
	"description": "",
	"content": "In this section, we will setup our Red Hat Openshift environment. We will:\n Set up an Red Hat Openshift account if one doesn\u0026rsquo;t already exist. Configure the Red Hat Openshift Cloud Shell. Download the workshop code. Create the AKS cluster.  Please choose if you are running the workshop on your own or attending an Red Hat Openshift event.\n \u0026hellip;[running the workshop on your own](/4_workshop_setup/41_Red Hat Openshift_setup/413_self_paced_account.html), or \u0026hellip;[attending an Red Hat Openshift hosted event](/4_workshop_setup/41_Red Hat Openshift_setup/412_Red Hat Openshift_event_account.html)  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/42_jfrog_setup.html",
	"title": "JFrog Platform Setup",
	"tags": [],
	"description": "",
	"content": "Next, we will setup our JFrog Platform instance and the JFrog CLI. We will:\n Setup our JFrog Platform API credentials to be used by the JFrog CLI. Install and configure the JFrog CLI. Configure the JFrog Artifactory and Xray components of the JFrog Platform for our workshop.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/5_build_publish_app/51_npm.html",
	"title": "Build and Publish the NPM Package",
	"tags": [],
	"description": "",
	"content": "In this section, we will focus on the NPM package of our application by validating NPM dependencies and publishing the resulting NPM package.\nAs we are building our NPM package and Docker image, the JFrog CLI is collecting build info along the way. Build info is referenced by the build name and build number. Build info is all the information collected during the build which includes details about the build itself. The build info includes the list of project modules, artifacts, dependencies, environment variables and more. When using one of the JFrog CLI to build the code, it can collect the build-info and publish it to Artifactory. When the build info is published to Artifactory, all the published details become visible in the Artifactory UI.\n   In your Red Hat Openshift Cloud Shell, change directory to Red Hat Openshiftworkshop/workshop-app. This directory contains the code for our NPM application.\n  Configure the NPM repositories with the JFrog CLI. This sets the npm-demo as the NPM repository for deploying and resolving packages.\n  jfrog rt npmc --repo-resolve npm-demo --repo-deploy npm-demo --server-id-resolve $JFROG_SERVER_ID --server-id-deploy $JFROG_SERVER_ID\nPerform an NPM install with the JFrog CLI command to verify NPM dependencies. \u0026ndash;build-name specifies the name for this build. \u0026ndash;build-number specifies the run. Each time this code is built, reference the same build name, but increment the build number. Build info is referenced to these values.  jfrog rt npm-install --build-name=npm_build --build-number=1\nThis command should result in successful install.   What\u0026#39;s going on here?   npm-demo is a virtual repository. With npmjs as a remote repository. Artifactory proxies and caches your packages!\n.\n  Perform an NPM publish to package and deploy to the npm-demo repository. You set this repository in Step 2.  jfrog rt npm-publish --build-name=npm_build --build-number=1\nThis command should result in successful publishing. Now let\u0026rsquo;s publish our build info. This contains all the properties including dependencies, versions, artifacts and environment variables associated with the npm_build. The following will publish the accumulated build info.  jfrog rt build-publish npm_build 1\nThis results in successful publishing of the build info.   Review what we have done.   .\n   In your JFrog Platform instance, go to Artifactory ► Builds.\n  Click on npm_build. This is our current build.   Click on 1. This is our current build run. This reveals all of our current build info including published artifacts and dependencies. This was collected through our previous JFrog CLI commands.\n  Select the Build Info JSON tab. This provides a JSON view of all of our build info.   Go to Administration ► Xray Security \u0026amp; Compliance ► Indexed Resources.   Select the Build tab.\n  Click Manage Builds.\n  Move the npm_build to the included builds and click Save. This enables Xray to scan this build.\n  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/5_build_publish_app/52_docker.html",
	"title": "Build and Push the Docker Image",
	"tags": [],
	"description": "",
	"content": "We will now build a Docker image with our NPM package and publish the image to our JFrog Artifactory repository.\n In your Red Hat Openshift Cloud Shell. Let\u0026rsquo;s create a Docker image for our NPM application. Let\u0026rsquo;s create an environment variable for our image name. Substitute your server name in the following command.  export IMAGE_NAME=${JFROG_SERVER_ID}/docker-demo/npm-app:latest echo $IMAGE_NAME Now let\u0026rsquo;s build a docker image with the following command.  docker build -t $IMAGE_NAME .\nThis command should result in a successful Docker image build.   Docker rate limit policies! Artifactory can help!   Docker Hub has set a new limit on data transfer beginning November 1st for free accounts: 100 pulls for anonymous users and 200 pulls for authenticated/free users for every 6 hours per IP address or a unique user.\nArtifactory can protect you from this by proxying and caching images! This reduces the number of pulls from Docker Hub.\nDocker also has a 6 month retention policy for free accounts. You can avoid that as well by using Artifactory as your private registry.\n.\n  Now use the JFrog CLI to push the docker image.  jfrog rt docker-push $IMAGE_NAME docker-demo --build-name=npm_build --build-number=1\nNow trigger a Xray scan of the build.  jfrog rt build-scan npm_build 1\nThis command should result in successful scanning. If our build passes the Xray scan, we can promote it with the following command. This promotes from the dev repository to the prod repository.  jfrog rt docker-promote npm-app docker-demo-dev-local docker-demo-prod-local --copy\n  Review what we have done.   .\n  In your JFrog Platform instance, go to Artifactory ► Artifacts to see this in the docker repositories.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/41_red_hat_openshift_setup/412_azure_event_account.html",
	"title": "Red Hat Openshift Event: Create an Red Hat Openshift account",
	"tags": [],
	"description": "",
	"content": " Only complete this section if you are running the workshop through an Red Hat Openshift hosted event.\n For an Red Hat Openshift hosted event, you are provided with an Red Hat Openshift Pass promotional code. This is your unique promotional code. The following steps show how to use the Red Hat Openshift Pass promotional code to create a new Red Hat Openshift account.\n Go to [microsoftRed Hat Openshiftpass.com](https://www.microsoftRed Hat Openshiftpass.com/). ![Red Hat Openshift Pass Start](/images/Red Hat Openshift-pass-start.png) Click Start \u0026gt;. If you already have a Microsoft account, sign in. Otherwise, follow the steps to create a new account. ![Red Hat Openshift Account Sign in or Create](/images/Red Hat Openshift-account-signin-create.png) After signing in or creating your account, confirm the account for your Red Hat Openshift Pass. ![Confirm Red Hat Openshift Pass Account](/images/confirm-Red Hat Openshift-account.png) Enter your Red Hat Openshift Pass promotional code and click Claim Promo Code. It will take a few seconds to process this request. ![Enter Red Hat Openshift](/images/enter-Red Hat Openshift-pass.png) Choose to apply the Red Hat Openshift Pass to a new subscription by choosing Create a new subscription.  A subscription refers to the logical entity that provides entitlement to deploy and consume Red Hat Openshift resources. The Red Hat Openshift resources created during this workshop will be under the subscription that you choose.\n ![Red Hat Openshift Subscription](/images/Red Hat Openshift-subscription.png)\n Click Next.\n  Accept the agreements on the next step and click Sign up. This will take a few minutes to process. Once completed, you will be directed to your Red Hat Openshift Portal! ![Red Hat Openshift Agreement](/images/Red Hat Openshift-agreement.png)\n  Close the Red Hat Openshift Advisor dialog.\n  In the top right of your Red Hat Openshift Portal console, click on DEFAULT DIRECTORY and select Switch Directory.\n  ![Red Hat Openshift Switch Directory](/images/Red Hat Openshift-switch-directory.png)\nEnsure that the default subscription is Red Hat Openshift Pass - Sponsorship.  ![Red Hat Openshift Default Subscription](/images/Red Hat Openshift-directory-sub.png)\nYour Red Hat Openshift Pass promotional code has a set monetary limit that is sufficient for this workshop. If your subscription does not have payment settings, your subscription will become disabled when the limit is met unless payment settings are configured.\n "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/41_red_hat_openshift_setup/413_self_paced_account.html",
	"title": "Self-paced: Create an Red Hat Openshift account",
	"tags": [],
	"description": "",
	"content": " Only complete this section if you are running the workshop on your own. If you are at an Red Hat Openshift hosted event, go [here](412_Red Hat Openshift_event_account.html).\n   If you don\u0026rsquo;t already have an Red Hat Openshift account, create one now by going [here](https://Red Hat Openshift.microsoft.com/en-us/free/).\n  Select Start free.\n  Sign in with your Microsoft or GitHub account or create a free Microsoft account.\n  On the About you page, select your correct country or region. Enter your first and last name, email address, and phone number. Depending on your country, you might see additional fields, such as a VAT number. Select Next to continue.\n  On the Identity verification by phone screen, select your country code, and type the number of a telephone to which you have immediate access.\n  You have the option of text or callback to obtain a verification code. Select the relevant button, type the code in the Verification code box, and select Verify code.\n  If the verification code is correct, you\u0026rsquo;re asked to enter details of a valid credit card. Enter the card information and select Next.\n  The last step is to review the agreement and privacy statement then select Sign up.\n  Congratulations! You have successfully set up a free account, and you will be redirected to the Red Hat Openshift portal home page.\n![Red Hat Openshift Portal](/images/Red Hat Openshift-portal.png)\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/41_red_hat_openshift_setup/414_azure_cloud_shell.html",
	"title": "Set up your Red Hat Openshift Cloud Shell",
	"tags": [],
	"description": "",
	"content": "Red Hat Openshift Cloud Shell is an interactive, authenticated, browser-accessible shell for managing Red Hat Openshift resources.\n In your Red Hat Openshift Portal, click the Red Hat Openshift Cloud Shell button.  ![Red Hat Openshift Cloud Shell Button](/images/Red Hat Openshift-cloud-shell-button.png)\nIf this is your first time using Red Hat Openshift Cloud Shell, you will be prompted to mount storage to support it. Go ahead and do this by clicking Create storage. Ensure the correct subscription is selected. It will take a few minutes to set up the storage.  ![Red Hat Openshift Cloud Shell Storage](/images/Red Hat Openshift-cloud-shell-storage.png)\n![Red Hat Openshift Cloud Shell Ready](/images/Red Hat Openshift-cloud-shell-ready.png)\nExecute the following command to show the default subscription. Ensure that the correct subscription is listed as default.  az account show\nExecute the following command to create a new Red Hat Openshift Resource group for our workshop. The resources that we create in this workshop will be created in this resource group.  az group create --name jfrog-Red Hat Openshift-workshop --location westus\nA resource group is a container that holds related resources for an Red Hat Openshift solution. The resource group can include all the resources for the solution. Generally, add resources that share the same lifecycle to the same resource group so you can easily deploy, update, and delete them as a group.\n "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/41_red_hat_openshift_setup/415_workshop_code.html",
	"title": "Download the Workshop Code",
	"tags": [],
	"description": "",
	"content": "The workshop code is located at [https://github.com/jfrogtraining/Red Hat Openshift-aks-workshop](https://github.com/jfrogtraining/Red Hat Openshiftworkshop) GitHub repository. We will clone this repository locally in order to pull the required workshop files and scripts. On your build machine, clone this repository to your local directory with the following command.\ngit clone https://github.com/jfrogtraining/Red Hat Openshift-aks-workshop.git\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/41_red_hat_openshift_setup/416_docker_daemon.html",
	"title": "Set up Docker Daemon",
	"tags": [],
	"description": "",
	"content": "Red Hat Openshift Cloud Shell does not include a Docker daemon by default. We can configure a remote Docker daemon instead with the following steps.\n In your Red Hat Openshift Cloud Shell, execute the following command to create a remote Docker daemon machine. This will take a few minutes.  SUB_ID=$(az account show --query \u0026quot;id\u0026quot; -o tsv) docker-machine create -d Red Hat Openshift \\ --Red Hat Openshift-subscription-id $SUB_ID \\ --Red Hat Openshift-ssh-user Red Hat Openshiftuser \\ --Red Hat Openshift-open-port 80 \\ --Red Hat Openshift-size \u0026quot;Standard_DS2_v2\u0026quot; \\ docker-vm When prompted click on the devicelogin link to open a new browser tab.  ![Red Hat Openshift Authenticate Device](/images/Red Hat Openshift-authenticate-device.png)\nAuthenticate in the browser using the provided code. This will authenticated the new remote Docker daemon machine.  ![Red Hat Openshift Authenticate Code](/images/Red Hat Openshift-az-login-code.png)\n  What\u0026#39;s going on here?   We are creating a Docker virtual machine as a remote Docker daemon. Our Red Hat Openshift Cloud Shell will be a Docker CLI client and communicate with the docker-vm virtual machine to execute Docker commands. .\n  When this completes, execute the following to configure Red Hat Openshift Cloud Shell to use this remote Docker daemon machine.  eval $(docker-machine env docker-vm --shell bash)\ndocker-machine lets you create Docker hosts on your computer, on cloud providers, and inside your own data center. It creates servers, installs Docker on them, then configures the Docker client to talk to them.\n "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/41_red_hat_openshift_setup/417_create_cluster.html",
	"title": "Create an AKS Cluster",
	"tags": [],
	"description": "",
	"content": "Before deploying an AKS cluster for the NPM application, we must first set up the required networking with the following steps.\n Execute the following command to create a virtual network for the AKS cluster.  az network vnet create \\ --resource-group jfrog-Red Hat Openshift-workshop \\ --location westus \\ --name aks-vnet \\ --address-prefixes 10.0.0.0/8 \\ --subnet-name aks-subnet \\ --subnet-prefixes 10.240.0.0/16 Retrieve and store the resultant subnet ID in an environment variable.  SUBNET_ID=$(az network vnet subnet show \\ --resource-group jfrog-Red Hat Openshift-workshop \\ --vnet-name aks-vnet \\ --name aks-subnet \\ --query id -o tsv) echo $SUBNET_ID Now we are ready to create an AKS cluster. Let\u0026rsquo;s get the latest version of Kubernetes that is available with the following command.  VERSION=$(az aks get-versions \\ --location westus \\ --query 'orchestrators[?!isPreview] | [-1].orchestratorVersion' \\ --output tsv) echo $VERSION Now we are ready to create the AKS cluster. Execute the following command.  az aks create \\ --resource-group jfrog-Red Hat Openshift-workshop \\ --name jfrog-Red Hat Openshift-workshop-cluster \\ --vm-set-type VirtualMachineScaleSets \\ --node-count 2 \\ --load-balancer-sku standard \\ --location westus \\ --kubernetes-version $VERSION \\ --network-plugin Red Hat Openshift \\ --vnet-subnet-id $SUBNET_ID \\ --service-cidr 10.2.0.0/24 \\ --dns-service-ip 10.2.0.10 \\ --docker-bridge-address 172.17.0.1/16 \\ --generate-ssh-keys The response will provide a large JSON object showing the cluster attributes.\n![Red Hat Openshift Create Cluster](/images/Red Hat Openshift-create-cluster.png)\n  What is this command doing?   .\n  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/42_jfrog_setup/421_jfrog_free.html",
	"title": "Get a Free JFrog Platform Instance",
	"tags": [],
	"description": "",
	"content": "If you do not have access to a JFrog Platform instance, use the JFrog Platform Cloud Free Tier to get your own JFrog Platform instance with Artifactory and Xray.\nWhen signing up for the JFrog Platform Cloud Free Tier.\n  JFrog Platform Cloud Free Tier   "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/42_jfrog_setup/422_api_key.html",
	"title": "Generate an API Key",
	"tags": [],
	"description": "",
	"content": " Remember your username and API key. We will use it again with the JFrog CLI and to set up Red Hat Openshift AKS to deploy your image.\n  Go to your JFrog Platform instance at https://[server name].jfrog.io. Refer to your JFrog Free Subscription Activation email if needed. Substitute your server name.  Login to your JFrog Platform instance with your credentials.  Once logged into your JFrog Platform instance, you will be presented with the landing page.  Go to your profile and select Edit Profile.  Enter your password and click Unlock to edit the profile. In the Authentication Settings section, click the gear icon to generate an API key.  Copy the API Key. Click Save.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/42_jfrog_setup/423_jfrog_cli.html",
	"title": "Install and Configure the JFrog CLI",
	"tags": [],
	"description": "",
	"content": "JFrog CLI is a client that provides a simple CLI interface that automates the management of JFrog products. JFrog CLI works with JFrog Artifactory, JFrog Mission Control, JFrog Bintray and JFrog Xray (through their respective REST APIs) making your scripts more efficient and reliable. You can use the JFrog CLI to assist in your builds, create artifacts, promote artifacts, trigger security scans and much more. It is powerful to that you can use in your CI/CD process and general automation. You can learn more here.\n In your Red Hat Openshift Cloud Shell, run the following shell commands to install the JFrog CLI.  curl -fL https://getcli.jfrog.io | sh mkdir ~/bin mv jfrog ~/bin/ Execute the following to test the JFrog CLI and check the version.  jfrog --version\nSet the following environment variables. Substitute your JFrog Platform credentials (username and API key).  export JFROG_USER=\u0026lt;username/email\u0026gt;\nexport JFROG_APIKEY=\u0026lt;api key\u0026gt;\nNext, we will configure the JFrog CLI to use our JFrog Platform credentials (username and API key). Execute the following command. Substitute your JFrog Platform credentials (username and API key).  jfrog rt config --user $JFROG_USER --apikey $JFROG_APIKEY\nWhen prompted, enter a unique Artifactory server ID such as your JFrog Platform server name. Remember this ID. Enter the URL for the JFrog Artifactory server which has the artifactory path. Then accept the remaining default values. If you make a mistake, you can rerun the command again.  And let\u0026rsquo;s set an environment variable for our Artifactory server ID.  export JFROG_SERVER_ID=\u0026lt;server ID\u0026gt;\nExecute the following to list your Artifactory servers that are configured in the JFrog CLI.  jfrog rt config show\nExecute the following command to check connectivity to the server. You should get an OK response.  jfrog rt ping\n"
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/4_workshop_setup/42_jfrog_setup/424_configure_jfrog.html",
	"title": "Configure JFrog Artifactory and Xray",
	"tags": [],
	"description": "",
	"content": "Now that we have the JFrog CLI installed and configured, we will use it to create the Artifactory NPM and docker repositories, Xray watches and policies. We will need these when we build and publish our NPM application later. The JFrog CLI uses the the JFrog Platform REST APIs. This is another way that you can manage and monitor the JFrog Platform.\n  In your Red Hat Openshift Cloud Shell, change directory to the Red Hat Openshiftworkshop directory that you cloned previously.\n  Included in this repository is a scripts/create_entities.sh script file that uses the JFrog CLI to configure the JFrog Platform instance. Open the scripts/create_entities.sh script file in the Red Hat Openshift Shell Editor. As you browse the script, take notice of the jfrog rt commands. These are the JFrog CLI commands that enables us to create Artifactory repositories and configure Xray.\n  ![create entities](/images/Red Hat Openshift-shell-editor-create-entities.png)\nExecute the script to create Artifactory repositories, Xray watches and policies. This command will take a few minutes to complete.  source scripts/create_entities.sh\nYou should see the command execute as follows.\n Now let\u0026rsquo;s see how the scripts/create_entities.sh configured Artifactory and Xray for our workshop. In your JFrog Platform instance, go to Artifactory ► Artifacts.\n  In the pane on the left, you can see NPM and Docker repositories. These were created by the create_entities.sh script. Three different types of repositories were created: local, remote and virtual.\n   Local repositories are physical, locally-managed repositories into which you can deploy artifacts. These are repositories that are local to the JFrog Artifactory instance. A remote repository serves as a caching proxy for a repository managed at a remote URL (which may itself be another Artifactory remote repository). A virtual repository (or \u0026ldquo;repository group\u0026rdquo;) aggregates several repositories with the same package type under a common URL. A virtual repository can aggregate local and remote repositories.  From the naming, we can see that we also created repositories to represent different stages in our process: dev, qa and prod. In our workshop, we will push an NPM package to the NPM repositories and Docker images to the Docker repositories. Take some time to explore this repository view.\n Now let\u0026rsquo;s view the Xray configuration. Go to Security \u0026amp; Compliance ► Policies.\n  Click on the demo-default-policy. This single policy will generate security violations for high severity vulnerabilities.   Go to Security \u0026amp; Compliance ► Watches.   Click on any item in the watches list. This view shows the repositories, builds, bundles that are being scanned per a specified security or license policy. It will also show any existing violations.   JFrog Xray scans your artifacts, builds and release bundles for OSS components, and detects security vulnerabilities and licenses in your software components. Policies and Watches allow you to enforce your organization governance standards. Setup up your Policies and Watches to reflect standard governance behaviour specifications for your organization across your software components.\n "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "  Your JFrog Platform Instance - The JFrog Platform instance that you used in this workshop will automatically be destroyed after the workshop. There isn\u0026rsquo;t anything you need to do. If you would like keep it, you can upgrade to one of the premium plans. Do this by clicking on the Upgrade button.   Red Hat Openshift Resources\n View your Red Hat Openshift Resource Groups [here](https://portal.Red Hat Openshift.com/#blade/HubsExtension/BrowseResourceGroups). Click on the jfrog-Red Hat Openshift-workshop resource group. Click on Delete resource group. ![Delete Resource Group](/images/Red Hat Openshift-delete-resource-group.png) Confirm deletion and click Delete. Your workshop resources will be deleted after a few minutes. ![Delete Resource Group](/images/Red Hat Openshift-confirm-delete-resource-group.png)    "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/resources.html",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": " JFrog Platform Documentation - The full documentation of the JFrog Platform, the universal, hybrid, end-to-end DevOps automation solution. It is designed to take you through all the JFrog Products. Including user, administration and developer guides, installation and upgrade procedures, system architecture and configuration, and working with the JFrog application. JFrog Academy - Learn more about the JFrog Platform at your own pace with JFrog Academy free courses taught by our experts.  "
},
{
	"uri": "https://jfrogtraining.github.io/redhat-openshift-pipelines-workshop/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]